{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import torch\n",
    "from torch import nn\n",
    "import time\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import warnings\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, confusion_matrix, roc_auc_score, precision_score, recall_score\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('C:/Users/sande/OneDrive/Documents/SKOLE/master/pre/train2_meta.csv', index_col=0)\n",
    "train_signal = pd.read_csv('C:/Users/sande/OneDrive/Documents/SKOLE/master/pre/train2_signal.csv')\n",
    "valid_df = pd.read_csv('C:/Users/sande/OneDrive/Documents/SKOLE/master/pre/valid2_meta.csv', index_col=0)\n",
    "valid_signal = pd.read_csv('C:/Users/sande/OneDrive/Documents/SKOLE/master/pre/valid2_signal.csv')\n",
    "test_df = pd.read_csv('C:/Users/sande/OneDrive/Documents/SKOLE/master/pre/test2_meta.csv', index_col=0)\n",
    "test_signal = pd.read_csv('C:/Users/sande/OneDrive/Documents/SKOLE/master/pre/test2_signal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBXLDatasetPreprocesser():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def save(self, filename):\n",
    "        data = {\n",
    "            'class_cols': self.class_cols,    \n",
    "            'meta_num_cols': self.meta_num_cols,\n",
    "            'meta_num_means': self.meta_num_means,\n",
    "            'min_max_scaler': self.min_max_scaler,\n",
    "            'meta_cat_cols': self.meta_cat_cols,\n",
    "            'cat_lablers': self.cat_lablers,\n",
    "        }\n",
    "        pd.to_pickle(data, filename)\n",
    "        \n",
    "    def load(self, filename):\n",
    "        data = pd.read_pickle(filename)\n",
    "        self.min_max_scaler = data['min_max_scaler']\n",
    "        self.cat_lablers = data['cat_lablers']\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        x = x.copy()\n",
    "        y = y.copy()\n",
    "        \n",
    "        self.class_cols = ['NORM', 'MI', 'STTC', 'CD', 'HYP']\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.meta_num_cols = ['age', 'height', 'weight']\n",
    "        self.meta_num_means = []\n",
    "        for col in self.meta_num_cols:\n",
    "            print(col, y[col].mean())\n",
    "            y[col] = y[col].fillna(y[col].mean())\n",
    "            self.meta_num_means += [y[col].mean()]\n",
    "            \n",
    "        self.min_max_scaler = MinMaxScaler().fit(y[self.meta_num_cols])\n",
    "        \n",
    "        self.meta_cat_cols = ['sex'] #, 'nurse', 'device']\n",
    "        self.cat_lablers = [LabelEncoder().fit(y[col].fillna('none').astype(str)) for col in self.meta_cat_cols]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y):\n",
    "        \n",
    "        channel_cols = x.columns.tolist()[1:]\n",
    "        \n",
    "        ret = []\n",
    "        x = x[channel_cols].values.reshape(-1, 1000, 12)\n",
    "        print(x.shape)\n",
    "        ret += [x] # signal\n",
    "        \n",
    "        y_ = y.copy()\n",
    "        \n",
    "        for i, col in enumerate(self.meta_num_cols):\n",
    "            y_[col] = y_[col].fillna(self.meta_num_means[i])\n",
    "        y_[self.meta_num_cols] = self.min_max_scaler.transform(y_[self.meta_num_cols])\n",
    "        y_[self.meta_num_cols] = np.clip(y_[self.meta_num_cols], 0., 1.) # prevent extreme value far from train set\n",
    "        \n",
    "        ret += [y_[self.meta_num_cols]] # meta num features\n",
    "        \n",
    "        for i, col in enumerate(self.meta_cat_cols):\n",
    "            y_[col] = y_[col].fillna('none').astype(str)\n",
    "            y_[col] = self.cat_lablers[i].transform(y_[col]) \n",
    "        \n",
    "        ret += [y_[self.meta_cat_cols]] # meta cat features\n",
    "        \n",
    "        if np.isin(self.class_cols, y.columns).sum() == len(self.class_cols):\n",
    "            ret += [y[self.class_cols].fillna(0).astype(int)] # class targets\n",
    "        \n",
    "     \n",
    "        \n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessor = PTBXLDatasetPreprocesser()\n",
    "data_preprocessor.fit(train_signal, train_df)\n",
    "train_signal, train_meta_num_feats, train_meta_cat_feats, train_class  = data_preprocessor.transform(train_signal, train_df)\n",
    "valid_signal, valid_meta_num_feats, valid_meta_cat_feats, valid_class  = data_preprocessor.transform(valid_signal, valid_df)\n",
    "test_signal, test_meta_num_feats, test_meta_cat_feats, test_class  = data_preprocessor.transform(test_signal, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, signals, num_metas, cat_metas, class_labels=None):\n",
    "        self.signals = signals\n",
    "        self.num_metas = num_metas\n",
    "        self.cat_metas = cat_metas\n",
    "        self.class_labels = class_labels\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.signals.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        ret = []\n",
    "        ret += [self.signals[idx,:]]\n",
    "        ret += [self.num_metas.values[idx,:]]\n",
    "        ret += [self.cat_metas.values[idx,:]]\n",
    "        \n",
    "        if self.class_labels is not None:\n",
    "            ret += [self.class_labels.values[idx,:]]        \n",
    "\n",
    "        \n",
    "        return ret\n",
    "    \n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, signal_channel_size, lstm_hidden_size, per_cat_nunique, embed_size, num_size, hidden, n_outs):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.lstm1 = nn.LSTM(signal_channel_size, lstm_hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.rnn2 = nn.RNN(signal_channel_size, lstm_hidden_size, batch_first=True, bidirectional=True)\n",
    "        #self.gru3 = nn.GRU(signal_channel_size, lstm_hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.embeds = []\n",
    "        self.per_cat_nunique = per_cat_nunique\n",
    "        for v in self.per_cat_nunique:\n",
    "            self.embeds += [nn.Embedding(v, embed_size)]\n",
    "        self.embeds = nn.ModuleList(self.embeds)\n",
    "\n",
    "        self.dense1 = nn.Linear(lstm_hidden_size*4 + embed_size*len(per_cat_nunique) + num_size, hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out = nn.Linear(hidden, n_outs)\n",
    "\n",
    "    def forward(self, signal, num_meta, cat_meta):\n",
    "        signal = signal.view(signal.shape[0], signal.shape[1], -1)\n",
    "        #signal, _ = self.lstm1(signal)\n",
    "        signal, _ = self.rnn2(signal)\n",
    "        #signal, _ = self.gru3(signal)\n",
    "\n",
    "        avg_pool = torch.mean(signal, 1)\n",
    "        max_pool, _ = torch.max(signal, 1)\n",
    "\n",
    "        cat_feats = []\n",
    "        for i, embed in enumerate(self.embeds):\n",
    "            cat_feats += [embed(cat_meta[:,i].long())]\n",
    "        cat_feats = torch.cat(cat_feats, 1)\n",
    "\n",
    "        x = torch.cat([avg_pool, max_pool, cat_feats, num_meta], 1)\n",
    "        x = self.dense1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def prepare_dataloader(signal, meta_num_feats, meta_cat_feats, targetclass):\n",
    "    \n",
    "    ds = ECGDataset(signal, meta_num_feats, meta_cat_feats, class_labels=targetclass)\n",
    "    \n",
    "    dl = torch.utils.data.DataLoader(\n",
    "        ds,\n",
    "        batch_size=128,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        shuffle=True,        \n",
    "        num_workers=0,\n",
    "        \n",
    "    )\n",
    "    return dl\n",
    "\n",
    "def train_one_epoch(epoch, model, loss_fn, optimizer, train_loader, device, scheduler=None, schd_batch_update=False):\n",
    "    model.train()\n",
    "\n",
    "    t = time.time()\n",
    "    running_loss = None\n",
    "\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for step, (signal, num_meta, cat_meta, class_labels) in pbar:\n",
    "        signal = signal.to(device).float()\n",
    "        num_meta = num_meta.to(device).float()\n",
    "        cat_meta = cat_meta.to(device).long()\n",
    "        \n",
    "        class_labels = class_labels.to(device).long()\n",
    "        \n",
    "        labels = torch.cat([class_labels], 1).float()\n",
    "        \n",
    "        \n",
    "        with autocast():\n",
    "            preds = model(signal, num_meta, cat_meta)   \n",
    "            loss = loss_fn(preds, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if running_loss is None:\n",
    "                running_loss = loss.item()\n",
    "            else:\n",
    "                running_loss = running_loss * .99 + loss.item() * .01\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad() \n",
    "        wandb.log({ 'loss2': loss.item()})\n",
    "        if scheduler is not None and schd_batch_update:\n",
    "            scheduler.step()\n",
    "\n",
    "        description = f'epoch {epoch} loss: {running_loss:.4f}'\n",
    "\n",
    "        pbar.set_description(description)\n",
    "                \n",
    "    if scheduler is not None and not schd_batch_update:\n",
    "        scheduler.step()\n",
    "        \n",
    "def valid_one_epoch(epoch, model, loss_fn, val_loader, device, scheduler=None, schd_loss_update=False):\n",
    "    model.eval()\n",
    "\n",
    "    t = time.time()\n",
    "    loss_sum = 0\n",
    "    sample_num = 0\n",
    "    preds_all = []\n",
    "    targets_all = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(val_loader), total=len(val_loader))\n",
    "    for step, (signal, num_meta, cat_meta, class_labels) in pbar:\n",
    "        signal = signal.to(device).float()\n",
    "        num_meta = num_meta.to(device).float()\n",
    "        cat_meta = cat_meta.to(device).long()\n",
    "        \n",
    "        class_labels = class_labels.to(device)\n",
    "        \n",
    "        labels = torch.cat([class_labels], 1).float()\n",
    "        \n",
    "        preds = model(signal, num_meta, cat_meta)  \n",
    "        \n",
    "        preds_all += [preds.detach().cpu().numpy()]\n",
    "        targets_all += [labels.detach().cpu().numpy()]\n",
    "        \n",
    "        loss = loss_fn(preds, labels)\n",
    "        \n",
    "        loss_sum += loss.item()*labels.shape[0]\n",
    "        sample_num += labels.shape[0]  \n",
    "       \n",
    "        description = f'epoch {epoch} loss: {loss_sum/sample_num:.4f}'\n",
    "        pbar.set_description(description)\n",
    "    \n",
    "    preds_all = np.concatenate(preds_all)\n",
    "    targets_all = np.concatenate(targets_all)\n",
    "    class_cnt = class_labels.shape[1]\n",
    "    binary_preds_all = (preds_all >= 0.5)\n",
    "    precision = precision_score(targets_all, binary_preds_all, average='macro')\n",
    "    f1 = f1_score(targets_all, binary_preds_all, average='macro')\n",
    "    recall = recall_score(targets_all, binary_preds_all, average='macro')\n",
    "    conf_mat = confusion_matrix(targets_all[:,:class_cnt].argmax(axis=1), binary_preds_all[:,:class_cnt].argmax(axis=1))\n",
    "    \n",
    "    \n",
    "    wandb.log({'epoch': epoch, 'loss': loss_sum/sample_num, \n",
    "                'accuracy':(targets_all==(preds_all >= 0.5)).mean(), 'auc-roc':roc_auc_score(targets_all, preds_all, average='macro'),\n",
    "                  'precision': precision, 'F1 score': f1, 'recall': recall, 'Confusion matrix': conf_mat })\n",
    "    \n",
    "    if scheduler is not None:\n",
    "        if schd_loss_update:\n",
    "            scheduler.step(loss_sum/sample_num)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "            \n",
    "    return targets_all, preds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = prepare_dataloader(train_signal, train_meta_num_feats, train_meta_cat_feats, train_class )\n",
    "val_loader = prepare_dataloader(valid_signal, valid_meta_num_feats, valid_meta_cat_feats, valid_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    wandb.init(project=\"rnn2\")\n",
    "    \n",
    "    train_loader = prepare_dataloader(train_signal, train_meta_num_feats, train_meta_cat_feats, train_class)\n",
    "    val_loader = prepare_dataloader(valid_signal, valid_meta_num_feats, valid_meta_cat_feats, valid_class)\n",
    "    \n",
    "    epochs = 1\n",
    "    stepsize= 2\n",
    "    hidden_sie = 10\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    per_cat_nunique = [len(labeler.classes_) for labeler in data_preprocessor.cat_lablers]\n",
    "    model = LSTM(train_signal.shape[2], 128, per_cat_nunique, 30, train_meta_num_feats.shape[1], 128, \n",
    "                          train_class.shape[1]).to(device)\n",
    "    scaler = GradScaler()   \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, gamma=0.1, step_size=stepsize)\n",
    "\n",
    "    loss_tr = nn.BCEWithLogitsLoss().to(device)\n",
    "    loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_one_epoch(epoch, model, loss_tr, optimizer, train_loader, device, scheduler=scheduler, schd_batch_update=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_targets, val_preds = valid_one_epoch(epoch, model, loss_fn, val_loader, device, scheduler=None, schd_loss_update=False)\n",
    "\n",
    "    torch.save(model.state_dict(),'C:/Users/sande/OneDrive/Documents/SKOLE/master/pre/pytorch_ecg_rnn2.pth')\n",
    "\n",
    "    del model, optimizer, train_loader, val_loader, scaler, scheduler\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
