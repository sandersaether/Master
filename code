import os
import numpy as np
import pandas as pd
import wfdb
import torch

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.datasets as dset
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import wandb

wandb.init(project="ptbxl_lstm")
# Set the path to the PTB-XL dataset root directory
data_dir = 'C:/Users/sande/OneDrive/Documents/SKOLE/master/PTB-XL/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3/'
ptbxl_database = pd.read_csv(data_dir, index_col='ecg_id')
# Load the metadata CSV files into dataframes
metadata_df = pd.read_csv( 'C:/Users/sande/OneDrive/Documents/SKOLE/master/PTB-XL/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3/ptbxl_database.csv')
# Extract labels
labels = ptbxl_database[['diagnostic_class', 'diagnostic_subclass']]

# Extract the ECG record IDs from the metadata dataframe
record_ids = metadata_df['ecg_id'].values.tolist()

# Define the function to preprocess a single ECG record
def preprocess_record(record_id, sample_rate=100):
    # Load the ECG record from the WFDB format file
    record_path = os.path.join(data_dir, 'records100', record_id)
    signal, _ = wfdb.rdsamp(record_path)
    
    # Resample the ECG signal to the desired sample rate
    signal_resampled = signal[::int(signal.shape[0] / (sample_rate * 10)), :]
    
    # Extract the lead V1 signal from the ECG recording
    lead_v1 = signal_resampled[:, 7]
    
    # Normalize the lead V1 signal to have zero mean and unit variance
    lead_v1_norm = (lead_v1 - np.mean(lead_v1)) / np.std(lead_v1)
    
    return lead_v1_norm

# Define the function to preprocess multiple ECG records
def preprocess_records(record_ids, sample_rate=100):
    # Initialize an empty array to hold the preprocessed ECG signals
    signals = np.empty((len(record_ids), sample_rate * 10))
    
    # Loop over the ECG record IDs and preprocess each one
    for i, record_id in enumerate(record_ids):
        signal = preprocess_record(record_id, sample_rate)
        signals[i, :] = signal
    
    # Convert the preprocessed ECG signals to a PyTorch tensor
    signals_tensor = torch.from_numpy(signals).float()
    
    return signals_tensor

# Preprocess the ECG signals
signals_tensor = preprocess_records(record_ids)



# Convert the label annotations to a PyTorch tensor
labels_tensor = torch.from_numpy(labels)

# Create a PyTorch dataset from the preprocessed ECG signals and labels
dataset = torch.utils.data.TensorDataset(signals_tensor, labels_tensor)

# Split the dataset into training and test sets
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])




wandb.init(project="test")

# Set the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# Create the PyTorch dataloaders
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)


# Define the LSTM model
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(LSTMModel, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_classes = num_classes

        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, num_classes)

    def init_hidden(self, batch_size, num_layers, hidden_size):
        hx = torch.zeros(num_layers, batch_size, hidden_size).to(device)
        cx = torch.zeros(num_layers, batch_size, hidden_size).to(device)
        return hx, cx

    def forward(self, x):
        

        # Reshape the input tensor to a 3D tensor
        x = x.view(x.size(0), 1, -1)
        
        # Initialize the hidden state and cell state
        batch_size = x.size(0)
        hx, cx = self.init_hidden(batch_size, self.lstm.num_layers, self.hidden_size)

        # Forward pass through the LSTM layer
        out, (hx, cx) = self.lstm(x, (hx, cx))

        # Pass the final hidden state through the fully connected layer
        out = self.fc(hx[-1])

        return out

# Initialize the model and the optimizer
model = LSTMModel(input_size=12288, hidden_size=128, num_classes=2).to(device)
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Train the model
model.train()

for epoch in range(10):
    for i, (images, labels) in enumerate(train_loader):
        # Move the images and labels to the device
        images = images.to(device)
        labels = labels.to(device)

        # Forward pass
        outputs = model(images)

        # Calculate the loss
        loss = nn.CrossEntropyLoss()(outputs, labels)

        # Backward and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

         # Log the loss and accuracy to wandb
        wandb.log({'epoch': epoch, 'loss': loss.item()})
        # Print the loss every 100 steps
        if (i+1) % 100 == 0:
            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'
                  .format(epoch+1, 10, i+1, len(train_loader), loss.item()))

# Evaluate the model on the test set
model.eval()

with torch.no_grad():
    correct = 0
    total = 0
    for images, labels in test_loader:
        # Move the images and labels to the device
        images = images.to(device)
        labels = labels.to(device)

        # Forward pass
        outputs = model(images)

        # Get the predicted class from the outputs
        _, predicted = torch.max(outputs.data, 1)

        # Update the total and correct counts
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    # Print the test accuracy
    print('Test Accuracy: {:.2f}%'.format(100 * correct / total))
